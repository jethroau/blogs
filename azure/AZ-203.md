# AZ-203
[Developer learning path](https://docs.microsoft.com/en-us/learn/browse/?products=azure&roles=developer&resource_type=learning%20path)  
[Self-paced diagram](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RWtQqM)  

> 考试范围
> https://www.microsoft.com/zh-cn/learning/exam-AZ-203.aspx   

## Azure fundmental
1 Region, Geographies and Availability Zones


# Key content and Practice
## Azure Search
The process of creating an Azure Search index using C# and the .NET SDK. Index creation is accomplished by performing these tasks:
* Create a SearchServiceClient object to connect to a search service.  
* Create an Index object to pass as a parameter to Indexes.Create.  
* Call the Indexes.Create method on SearchServiceClient to send the Index to a service. 

Import data into an Azure Search index using C# and the .NET SDK. Pushing documents into your index is accomplished by performing these tasks:
* Create a SearchIndexClient object to connect to a search index. 
* Create an IndexBatch object containing the documents to be added, modified, or deleted. 
* Call the Documents.Index method on SearchIndexClient to upload documents to an index. 

> https://docs.microsoft.com/en-us/azure/search/search-import-data-dotnet 

## Azure Table Storage
Generates a property filter condition string for the string value.
```C#
public static string GenerateFilterCondition (string propertyName, string operation, string givenValue);

Parameters
propertyName
String
A string containing the name of the property to compare.

operation
String
A string containing the comparison operator to use.

givenValue
String
A string containing the value to compare with the property.

Returns
String
A string containing the formatted filter condition.
```
> https://docs.azure.cn/zh-cn/dotnet/api/microsoft.windowsazure.storage.table.tablequery.generatefiltercondition?view=azure-dotnet

## CosMosDB Consistency level 

* Strong: **Strong consistency offers a linearizability guarantee**. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write. 

* Bounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most "K" versions (i.e., "updates") of an item or by "T" time interval. In other words, when you choose bounded staleness, the "staleness" can be configured in two ways:  
The number of versions (K) of the item  
The time interval (T) by which the reads might lag behind the writes  

* Session: The reads are guaranteed to honor the consistent-prefix (assuming a single “writer” session), monotonic reads, monotonic writes, read-your-writes, and write-follows-reads guarantees. **Session consistency is scoped to a client session.** 

* Consistent prefix: Updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix consistency level guarantees that **reads never see out-of-order writes.** 

* Eventual: There's no ordering guarantee for reads. In the absence of any further writes, **the replicas eventually converge.** 
> https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels


## Azure Table Storage SDK (.NET)
**Create a Table**  
The CloudTableClient class enables you to retrieve tables and entities stored in Table storage. Because we don’t have any tables in the Cosmos DB Table API account, let’s add the CreateTableAsync method to the Common.cs class to create a table:
```C#
public static async Task<CloudTable> CreateTableAsync(string tableName)
  {
    // Create a table client for interacting with the table service
    CloudTableClient tableClient = storageAccount.CreateCloudTableClient(new TableClientConfiguration());
    CloudTable table = tableClient.GetTableReference(tableName);
    await table.CreateIfNotExistsAsync()
}
```
**Define the entity**  
This code defines an entity class that uses the customer's first name as the row key and last name as the partition key. Together,**an entity's partition and row key uniquely identify it in the table**. Entities with the same partition key can be queried faster than entities with different partition keys but using diverse partition keys allows for greater scalability of parallel operations. Entities to be stored in tables must be of a supported type, for example **derived from the TableEntity class**. Entity properties you'd like to store in a table must be public properties of the type, and support both getting and setting of values. Also, your entity type must expose a parameter-less constructor.
```C#
namespace CosmosTableSamples.Model
{
    using Microsoft.Azure.Cosmos.Table;
    public class CustomerEntity : TableEntity
    {
        public CustomerEntity()
        {
        }

        public CustomerEntity(string lastName, string firstName)
        {
            PartitionKey = lastName;
            RowKey = firstName;
        }

        public string Email { get; set; }
        public string PhoneNumber { get; set; }
    }
}
```

**Insert or merge an entity**  
The following code example creates an entity object and adds it to the table. The **InsertOrMerge** method within the **TableOperation** class is used to insert or merge an entity. The **CloudTable.ExecuteAsync** method is called to execute the operation.
```C#
public static async Task<CustomerEntity> InsertOrMergeEntityAsync(CloudTable table, CustomerEntity entity)    {
       // Create the InsertOrReplace table operation
       TableOperation insertOrMergeOperation = TableOperation.InsertOrMerge(entity);
       
       // Execute the operation.
       TableResult result = await table.ExecuteAsync(insertOrMergeOperation);
       
       CustomerEntity insertedCustomer = result.Result as CustomerEntity;
}
```

**Get an entity from a partition**  
You can get entity from a partition by using the Retrieve method under the TableOperation class. The following code example gets the partition key row key, email and phone number of a customer entity. 
```C#
public static async Task<CustomerEntity> RetrieveEntityUsingPointQueryAsync(CloudTable table, string partitionKey, string rowKey)
        
        TableOperation retrieveOperation = TableOperation.Retrieve<CustomerEntity>(partitionKey, rowKey);
        TableResult result = await table.ExecuteAsync(retrieveOperation);
        CustomerEntity customer = result.Result as CustomerEntity;

        return customer;
}
```

**Delete an entity**  
```C#
public static async Task DeleteEntityAsync(CloudTable table, CustomerEntity deleteEntity)
{
    TableOperation deleteOperation = TableOperation.Delete(deleteEntity);
    TableResult result = await table.ExecuteAsync(deleteOperation);
}
```
> https://docs.microsoft.com/en-us/azure/cosmos-db/tutorial-develop-table-dotnet


## Azure batch

### Create a pool of compute nodes 

#### .NET SDK
To create a Batch pool, the app uses the **BatchClient.PoolOperations.CreatePool** method to set the number of nodes, VM size, and a pool configuration.   
```c#
private static VirtualMachineConfiguration CreateVirtualMachineConfiguration(ImageReference imageReference)
{
    return new VirtualMachineConfiguration(
        imageReference: imageReference,
        nodeAgentSkuId: "batch.node.windows amd64");
}

private static ImageReference CreateImageReference()
{
    return new ImageReference(
        publisher: "MicrosoftWindowsServer",
        offer: "WindowsServer",
        sku: "2016-datacenter-smalldisk",
        version: "latest");
}

private static void CreateBatchPool(BatchClient batchClient, VirtualMachineConfiguration vmConfiguration)
{
    try
    {
        CloudPool pool = batchClient.PoolOperations.CreatePool(
            poolId: PoolId,
            targetDedicatedComputeNodes: PoolNodeCount,
            virtualMachineSize: PoolVMSize,
            virtualMachineConfiguration: vmConfiguration);

        pool.Commit();
    }
...
```
#### Python SDK
To create a Batch pool, the app uses the **PoolAddParameter** class to set the number of nodes, VM size, and a pool configuration.
```python
new_pool = batch.models.PoolAddParameter(
    id=pool_id,
    virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(
        image_reference=batchmodels.ImageReference(
            publisher="Canonical",
            offer="UbuntuServer",
            sku="18.04-LTS",
            version="latest"
            ),
        node_agent_sku_id="batch.node.ubuntu 18.04"),
    vm_size=config._POOL_VM_SIZE,
    target_dedicated_nodes=config._POOL_NODE_COUNT
)
batch_service_client.pool.add(new_pool)
```

### Create a Batch job
#### .NET SDK
The app uses the **BatchClient.JobOperations.CreateJob** method to create a job on your pool. The **Commit** method submits the job to the Batch service.   
```c#
try
{
    CloudJob job = batchClient.JobOperations.CreateJob();
    job.Id = JobId;
    job.PoolInformation = new PoolInformation { PoolId = PoolId };

    job.Commit();
}
...
```
#### Python SDK
The app uses the **JobAddParameter** class to create a job on your pool. The **job.add** method submits the pool to the Batch service.   

### Create tasks
#### .NET SDK
The app creates a list of **CloudTask** objects. Then, the app adds tasks to the job with the **AddTask** method, which queues them to run on the compute nodes.  `task.ResourceFiles`, `task.OutputFiles`, `OutputFileUploadCondition.TaskSuccess`, 
```c#
// Create a collection to hold the tasks added to the job.
List<CloudTask> tasks = new List<CloudTask>();

for (int i = 0; i < inputFiles.Count; i++)
{
    string taskId = String.Format("Task{0}", i);

    // Define task command line to convert each input file.
    string appPath = String.Format("%AZ_BATCH_APP_PACKAGE_{0}#{1}%", appPackageId, appPackageVersion);
    string inputMediaFile = inputFiles[i].FilePath;
    string outputMediaFile = String.Format("{0}{1}",
        System.IO.Path.GetFileNameWithoutExtension(inputMediaFile),
        ".mp3");
    string taskCommandLine = String.Format("cmd /c {0}\\ffmpeg-3.4-win64-static\\bin\\ffmpeg.exe -i {1} {2}", appPath, inputMediaFile, outputMediaFile);

    // Create a cloud task (with the task ID and command line)
    CloudTask task = new CloudTask(taskId, taskCommandLine);
    task.ResourceFiles = new List<ResourceFile> { inputFiles[i] };

    // Task output file
    List<OutputFile> outputFileList = new List<OutputFile>();
    OutputFileBlobContainerDestination outputContainer = new OutputFileBlobContainerDestination(outputContainerSasUrl);
    OutputFile outputFile = new OutputFile(outputMediaFile,
       new OutputFileDestination(outputContainer),
       new OutputFileUploadOptions(OutputFileUploadCondition.TaskSuccess));
    outputFileList.Add(outputFile);
    task.OutputFiles = outputFileList;
    tasks.Add(task);
}

// Add tasks as a collection
await batchClient.JobOperations.AddTaskAsync(jobId, tasks);
return tasks
```
#### Python SDK
The app creates a list of task objects using the **TaskAddParameter** class. Then, the app adds tasks to the job with the **task.add_collection** method, which queues them to run on the compute nodes.  

> https://docs.microsoft.com/en-us/azure/batch/quick-run-dotnet

## Deploy an Azure Kubernetes Service (AKS) cluster using the Azure CLI

### Create a resource group
```
az group create --name myResourceGroup --location eastus
```
### Create AKS cluster
```
az aks create \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --node-count 1 \
    --enable-addons monitoring \
    --generate-ssh-keys
```
### Connect to the cluster
```
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
kubectl get nodes
```
### Run the application
```
kubectl apply -f azure-vote.yaml
```
### Test the application
```
kubectl get service azure-vote-front --watch
```

> https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough

## Web App AutoScale
Step 1: Configure the web app to the Standard App Service Tier  
The Standard tier supports auto-scaling, and we should minimize the cost.  
Step 2: Enable autoscaling on the web app  
First enable autoscale  
Step 3: Add a scale rule  
Step 4: Add a Scale condidation  

> https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-get-started
> https://azure.microsoft.com/en-us/pricing/details/app-service/plans/

## Azure CDN overview
### How it works. 
1. A user (Alice) requests a file (also called an asset) by using a URL with a special domain name, such as <endpoint name>.azureedge.net. This name can be an endpoint hostname or a custom domain. The DNS routes the request to the best performing POP location, which is usually the POP that is geographically closest to the user.

2. If no edge servers in the POP have the file in their cache, the POP requests the file from the origin server. The origin server can be an Azure Web App, Azure Cloud Service, Azure Storage account, or any publicly accessible web server.

3. The origin server returns the file to an edge server in the POP.

4. An edge server in the POP caches the file and returns the file to the original requestor (Alice). The file remains cached on the edge server in the POP until the time-to-live (TTL) specified by its HTTP headers expires. If the origin server didn't specify a TTL, the default TTL is seven days.

5. Additional users can then request the same file by using the same URL that Alice used, and can also be directed to the same POP.

6. If the TTL for the file hasn't expired, the POP edge server returns the file directly from the cache. This process results in a faster, more responsive user experience.  

> https://docs.microsoft.com/en-us/azure/cdn/cdn-overview

## 


